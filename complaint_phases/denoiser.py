"""
Complaint Denoiser

Iteratively asks questions to fill gaps in the knowledge graph and reduce
noise/ambiguity in the complaint information.
"""

import logging
import re
from typing import Dict, List, Any, Optional, Tuple, Set
import os
import random
from .knowledge_graph import KnowledgeGraph, Entity, Relationship
from .dependency_graph import DependencyGraph

logger = logging.getLogger(__name__)


class ComplaintDenoiser:
    """
    Denoises complaint information through iterative questioning.
    
    Uses knowledge graph gaps and dependency graph requirements to generate
    targeted questions that help clarify and complete the complaint.
    """
    
    def __init__(self, mediator=None):
        self.mediator = mediator
        self.questions_asked = []
        self.questions_pool = []

        # Optional “policy” knobs for SGD-style exploration.
        # Default is deterministic/no-randomness.
        self.exploration_epsilon = self._env_float("CG_DENOISER_EXPLORATION_EPSILON", 0.0)
        self.momentum_beta = self._env_float("CG_DENOISER_MOMENTUM_BETA", 0.85)
        self.momentum_enabled = self._env_bool("CG_DENOISER_MOMENTUM_ENABLED", False)
        self.exploration_enabled = self._env_bool("CG_DENOISER_EXPLORATION_ENABLED", False)
        self.exploration_top_k = int(self._env_float("CG_DENOISER_EXPLORATION_TOP_K", 3) or 3)
        self.stagnation_window = int(self._env_float("CG_DENOISER_STAGNATION_WINDOW", 4) or 4)
        self.stagnation_gain_threshold = float(self._env_float("CG_DENOISER_STAGNATION_GAIN_THRESHOLD", 0.5) or 0.5)

        seed = os.getenv("CG_DENOISER_SEED")
        self._rng = random.Random(int(seed)) if seed and seed.isdigit() else random.Random()

        # Momentum state: EMA of “gain” by question type.
        self._type_gain_ema: Dict[str, float] = {}
        self._recent_gains: List[float] = []


    def _env_bool(self, key: str, default: bool) -> bool:
        raw = os.getenv(key)
        if raw is None:
            return default
        val = raw.strip().lower()
        if val in {"1", "true", "yes", "y", "on"}:
            return True
        if val in {"0", "false", "no", "n", "off"}:
            return False
        return default


    def _env_float(self, key: str, default: float) -> float:
        raw = os.getenv(key)
        if raw is None:
            return default
        try:
            return float(raw.strip())
        except Exception:
            return default


    def _short_description(self, text_value: str, limit: int = 160) -> str:
        snippet = " ".join((text_value or "").strip().split())
        if len(snippet) > limit:
            return snippet[: limit - 3] + "..."
        return snippet


    def _extract_date_strings(self, text: str) -> List[str]:
        if not text:
            return []
        date_patterns = [
            r'\b(?:Jan|January|Feb|February|Mar|March|Apr|April|May|Jun|June|Jul|July|Aug|August|Sep|Sept|September|Oct|October|Nov|November|Dec|December)\s+\d{1,2},\s+\d{4}\b',
            r'\b(?:Jan|January|Feb|February|Mar|March|Apr|April|May|Jun|June|Jul|July|Aug|August|Sep|Sept|September|Oct|October|Nov|November|Dec|December)\s+\d{4}\b',
            r'\b\d{1,2}/\d{1,2}/\d{2,4}\b',
            r'\b\d{4}-\d{2}-\d{2}\b',
            r'\b(?:in|on|since|during|around|by|from)\s+((?:19|20)\d{2})\b',
        ]
        found: List[str] = []
        for pattern in date_patterns:
            for match in re.findall(pattern, text):
                if isinstance(match, tuple):
                    match = match[0]
                value = (match or "").strip()
                if value and value not in found:
                    found.append(value)
        return found


    def _extract_org_candidates(self, text: str) -> List[str]:
        if not text:
            return []
        candidates: Set[str] = set()
        org_suffixes = (
            r"(?:Inc\.?|LLC|Ltd\.?|Co\.?|Corp\.?|Corporation|Company|University|"
            r"Hospital|School|Department|Dept\.?|Agency|Clinic|Bank|Foundation|"
            r"Association|Partners|Group|Systems|Services)"
        )
        for match in re.findall(
            rf"\b([A-Z][\w&.-]*(?:\s+[A-Z][\w&.-]*){{0,4}}\s+{org_suffixes})\b",
            text,
        ):
            candidates.add(match.strip())
        for match in re.findall(
            r"\b(?:at|for|with|from)\s+([A-Z][\w&.-]*(?:\s+[A-Z][\w&.-]*){0,4})\b",
            text or "",
        ):
            candidates.add(match.strip())

        lower_text = text.lower()
        if any(
            k in lower_text
            for k in [
                "property management",
                "property manager",
                "management company",
                "leasing office",
                "housing authority",
                "housing office",
            ]
        ):
            candidates.add("Property Management")
        if "employer" in lower_text:
            candidates.add("Employer")
        if any(
            k in lower_text
            for k in [
                "company",
                "organization",
                "business",
                "agency",
                "department",
                "school",
                "university",
                "hospital",
                "clinic",
            ]
        ):
            candidates.add("Organization")

        months = {
            "january",
            "february",
            "march",
            "april",
            "may",
            "june",
            "july",
            "august",
            "september",
            "october",
            "november",
            "december",
            "jan",
            "feb",
            "mar",
            "apr",
            "jun",
            "jul",
            "aug",
            "sep",
            "sept",
            "oct",
            "nov",
            "dec",
            "monday",
            "tuesday",
            "wednesday",
            "thursday",
            "friday",
            "saturday",
            "sunday",
        }
        banned = {"i", "we", "they", "he", "she", "it", "my", "our", "their", "the", "a", "an"}

        results: List[str] = []
        for candidate in sorted(candidates):
            cleaned = re.sub(r"[^\w&.\-\s]", "", candidate).strip()
            if not cleaned:
                continue
            lowered = cleaned.lower()
            if lowered in banned or lowered in months:
                continue
            tokens = cleaned.split()
            if len(tokens) == 1 and len(cleaned) < 4:
                continue
            results.append(cleaned)
        return results


    def _extract_named_role_people(self, text: str) -> List[Tuple[str, str]]:
        if not text:
            return []
        role_keywords = (
            r"manager|supervisor|boss|coworker|co-worker|hr|human resources|director|"
            r"owner|principal|teacher|professor|doctor|nurse|attorney|lawyer|agent|"
            r"officer|landlord|neighbor|representative"
        )
        results: List[Tuple[str, str]] = []
        for match in re.finditer(
            rf"\b(?:my|the|a|an)\s+(?P<role>{role_keywords})\s+(?P<name>[A-Z][a-z]+(?:\s+[A-Z][a-z]+){{0,2}})\b",
            text or "",
            re.IGNORECASE,
        ):
            role = (match.group("role") or "").strip().lower()
            name = (match.group("name") or "").strip()
            if name and role:
                results.append((name, role))
        return results


    def _extract_generic_roles(self, text: str) -> List[str]:
        if not text:
            return []
        lower_text = text.lower()
        roles = [
            "manager",
            "supervisor",
            "boss",
            "owner",
            "landlord",
            "hr",
            "human resources",
            "director",
            "agent",
            "representative",
            "officer",
            "employer",
            "company",
            "organization",
            "agency",
            "department",
            "school",
            "university",
            "hospital",
            "clinic",
        ]
        found: List[str] = []
        for role in roles:
            if role in lower_text and role not in found:
                found.append(role)
        return found


    def _contains_remedy_cue(self, text: str) -> bool:
        if not text:
            return False
        lowered = text.lower()
        cues = [
            "seeking",
            "seek",
            "would like",
            "looking for",
            "request",
            "asking for",
            "refund",
            "reimbursement",
            "compensation",
            "back pay",
            "repair",
            "fix",
            "replacement",
            "apology",
            "policy change",
        ]
        return any(cue in lowered for cue in cues)


    def _update_responsible_parties_from_answer(self,
                                               answer: str,
                                               knowledge_graph: KnowledgeGraph,
                                               updates: Dict[str, Any]) -> Dict[str, Any]:
        if not answer:
            return updates
        claims = knowledge_graph.get_entities_by_type("claim")
        claim_id = claims[0].id if len(claims) == 1 else None
        added_any = False

        for name, role in self._extract_named_role_people(answer):
            person, created = self._add_entity_if_missing(
                knowledge_graph,
                "person",
                name,
                {"role": role},
                0.7,
            )
            if created:
                updates["entities_updated"] += 1
            if claim_id and person:
                _, rel_created = self._add_relationship_if_missing(
                    knowledge_graph,
                    claim_id,
                    person.id,
                    "involves",
                    0.6,
                )
                if rel_created:
                    updates["relationships_added"] += 1
            added_any = True

        for org_name in self._extract_org_candidates(answer):
            org, created = self._add_entity_if_missing(
                knowledge_graph,
                "organization",
                org_name,
                {"role": "respondent"},
                0.6,
            )
            if created:
                updates["entities_updated"] += 1
            if claim_id and org:
                _, rel_created = self._add_relationship_if_missing(
                    knowledge_graph,
                    claim_id,
                    org.id,
                    "involves",
                    0.6,
                )
                if rel_created:
                    updates["relationships_added"] += 1
            added_any = True

        if not added_any:
            for role in self._extract_generic_roles(answer):
                role_norm = role.strip().lower()
                if role_norm in {"employer", "company", "organization", "agency", "department", "school", "university", "hospital", "clinic"}:
                    etype = "organization"
                    name = "Employer" if role_norm == "employer" else role_norm.title()
                    attrs = {"role": "respondent"}
                    confidence = 0.55
                else:
                    etype = "person"
                    name = "HR" if role_norm == "hr" else role_norm.title()
                    attrs = {"role": role_norm if role_norm != "human resources" else "hr"}
                    confidence = 0.55
                entity, created = self._add_entity_if_missing(
                    knowledge_graph,
                    etype,
                    name,
                    attrs,
                    confidence,
                )
                if created:
                    updates["entities_updated"] += 1
                if claim_id and entity:
                    _, rel_created = self._add_relationship_if_missing(
                        knowledge_graph,
                        claim_id,
                        entity.id,
                        "involves",
                        0.55,
                    )
                    if rel_created:
                        updates["relationships_added"] += 1
        return updates


    def _find_entity(self, knowledge_graph: KnowledgeGraph, etype: str, name: str) -> Optional[Entity]:
        etype_norm = (etype or "").strip().lower()
        name_norm = (name or "").strip().lower()
        if not etype_norm or not name_norm:
            return None
        for entity in knowledge_graph.entities.values():
            if entity.type.lower() == etype_norm and entity.name.strip().lower() == name_norm:
                return entity
        return None


    def _next_entity_id(self, knowledge_graph: KnowledgeGraph) -> str:
        max_id = 0
        for entity_id in knowledge_graph.entities.keys():
            match = re.match(r"entity_(\d+)$", str(entity_id))
            if match:
                max_id = max(max_id, int(match.group(1)))
        return f"entity_{max_id + 1}"


    def _next_relationship_id(self, knowledge_graph: KnowledgeGraph) -> str:
        max_id = 0
        for rel_id in knowledge_graph.relationships.keys():
            match = re.match(r"rel_(\d+)$", str(rel_id))
            if match:
                max_id = max(max_id, int(match.group(1)))
        return f"rel_{max_id + 1}"


    def _add_entity_if_missing(self,
                               knowledge_graph: KnowledgeGraph,
                               etype: str,
                               name: str,
                               attributes: Dict[str, Any],
                               confidence: float) -> Tuple[Optional[Entity], bool]:
        existing = self._find_entity(knowledge_graph, etype, name)
        if existing:
            return existing, False
        entity = Entity(
            id=self._next_entity_id(knowledge_graph),
            type=etype,
            name=name,
            attributes=attributes,
            confidence=confidence,
            source='complaint'
        )
        knowledge_graph.add_entity(entity)
        return entity, True


    def _add_relationship_if_missing(self,
                                    knowledge_graph: KnowledgeGraph,
                                    source_id: str,
                                    target_id: str,
                                    relation_type: str,
                                    confidence: float) -> Tuple[Optional[Relationship], bool]:
        if not (source_id and target_id and relation_type):
            return None, False
        for rel in knowledge_graph.relationships.values():
            if rel.source_id == source_id and rel.target_id == target_id and rel.relation_type == relation_type:
                return rel, False
        relationship = Relationship(
            id=self._next_relationship_id(knowledge_graph),
            source_id=source_id,
            target_id=target_id,
            relation_type=relation_type,
            attributes={},
            confidence=confidence,
            source='complaint'
        )
        knowledge_graph.add_relationship(relationship)
        return relationship, True


    def get_policy_state(self) -> Dict[str, Any]:
        return {
            "exploration_enabled": bool(self.exploration_enabled),
            "exploration_epsilon": float(self.exploration_epsilon),
            "exploration_top_k": int(self.exploration_top_k),
            "momentum_enabled": bool(self.momentum_enabled),
            "momentum_beta": float(self.momentum_beta),
            "stagnation_window": int(self.stagnation_window),
            "stagnation_gain_threshold": float(self.stagnation_gain_threshold),
            "type_gain_ema": dict(self._type_gain_ema),
            "recent_gains": list(self._recent_gains[-10:]),
        }


    def _compute_gain(self, updates: Dict[str, Any]) -> float:
        # A small heuristic: “did this question produce useful structured updates?”
        return float(
            (updates.get('entities_updated') or 0)
            + (updates.get('relationships_added') or 0)
            + (updates.get('requirements_satisfied') or 0)
        )


    def _update_momentum(self, question_type: str, gain: float) -> None:
        qtype = (question_type or "unknown").strip() or "unknown"
        prev = float(self._type_gain_ema.get(qtype, gain))
        beta = float(self.momentum_beta)
        beta = min(max(beta, 0.0), 0.999)
        self._type_gain_ema[qtype] = beta * prev + (1.0 - beta) * float(gain)


    def _maybe_increase_exploration_when_stuck(self) -> float:
        # If recent gains are consistently low, boost epsilon slightly.
        if self.stagnation_window <= 0:
            return float(self.exploration_epsilon)
        window = self._recent_gains[-self.stagnation_window :]
        if len(window) < self.stagnation_window:
            return float(self.exploration_epsilon)
        avg_gain = sum(window) / max(len(window), 1)
        if avg_gain <= self.stagnation_gain_threshold:
            return min(0.5, float(self.exploration_epsilon) + 0.1)
        return float(self.exploration_epsilon)


    def is_stagnating(self) -> bool:
        if self.stagnation_window <= 0:
            return False
        window = self._recent_gains[-self.stagnation_window :]
        if len(window) < self.stagnation_window:
            return False
        avg_gain = sum(window) / max(len(window), 1)
        return avg_gain <= self.stagnation_gain_threshold


    def _apply_exploration_and_momentum(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        if not questions:
            return questions

        # Keep deterministic behavior unless explicitly enabled.
        if not (self.momentum_enabled or self.exploration_enabled):
            return questions

        # Momentum: reorder within the same priority bucket by EMA gain.
        priority_order = {'high': 0, 'medium': 1, 'low': 2}

        def score(q: Dict[str, Any]) -> float:
            qtype = (q.get('type') or 'unknown')
            return float(self._type_gain_ema.get(qtype, 0.0))

        # Group by priority to preserve the high/medium/low structure.
        grouped: Dict[int, List[Dict[str, Any]]] = {0: [], 1: [], 2: [], 3: []}
        for q in questions:
            grouped[priority_order.get(q.get('priority', 'low'), 3)].append(q)

        if self.momentum_enabled:
            for k in list(grouped.keys()):
                grouped[k].sort(key=score, reverse=True)

        merged: List[Dict[str, Any]] = []
        for k in sorted(grouped.keys()):
            merged.extend(grouped[k])

        # Exploration: with probability epsilon, swap the top question with another
        # from the top-K to encourage exploration.
        if self.exploration_enabled and self.exploration_top_k > 1:
            epsilon = self._maybe_increase_exploration_when_stuck()
            if self._rng.random() < max(0.0, min(1.0, epsilon)):
                k = min(int(self.exploration_top_k), len(merged))
                if k > 1:
                    j = self._rng.randrange(0, k)
                    merged[0], merged[j] = merged[j], merged[0]

        return merged

    def _normalize_question_text(self, text: str) -> str:
        return (text or "").strip().lower()

    def _already_asked(self, question_text: str) -> bool:
        norm = self._normalize_question_text(question_text)
        for item in self.questions_asked:
            q = item.get('question') or {}
            if isinstance(q, dict):
                asked_text = q.get('question', '')
            else:
                asked_text = str(q)
            if self._normalize_question_text(asked_text) == norm:
                return True
        return False

    def _with_empathy(self, question_text: str, question_type: str) -> str:
        # Keep this minimal so we don't overwhelm the prompt.
        text = (question_text or "").strip()
        if not text:
            return text
        prefix = ""
        if question_type in {'clarification', 'relationship', 'requirement'}:
            prefix = "To make sure I understand, "
        elif question_type in {'evidence'}:
            prefix = "So we can support your claim, "
        if prefix and not text.lower().startswith(prefix.strip().lower()):
            return prefix + text[0].lower() + text[1:] if len(text) > 1 else prefix + text.lower()
        return text

    def _ensure_standard_intake_questions(self, questions: List[Dict[str, Any]], max_questions: int) -> List[Dict[str, Any]]:
        if len(questions) >= max_questions:
            return questions

        existing_text = " ".join([q.get('question', '') for q in questions]).lower()
        added: List[Dict[str, Any]] = []

        # Timeline question
        timeline_text = (
            "What is the timeline of key events (dates, who was involved, what was said or done, and when you requested help/accommodation)?"
        )
        if len(questions) + len(added) < max_questions:
            if not any(q.get('type') == 'timeline' for q in questions) and not any(k in existing_text for k in ['timeline', 'when did', 'what date', 'dates']):
                if not self._already_asked(timeline_text):
                    added.append({
                        'type': 'timeline',
                        'question': timeline_text,
                        'context': {},
                        'priority': 'high'
                    })

        # Harms/remedy question
        impact_text = (
            "What harm did you experience (financial, emotional, professional), and what outcome or remedy are you seeking?"
        )
        if len(questions) + len(added) < max_questions:
            if not any(q.get('type') in {'impact', 'remedy'} for q in questions) and not any(k in existing_text for k in ['harm', 'damages', 'remedy', 'seeking']):
                if not self._already_asked(impact_text):
                    added.append({
                        'type': 'impact',
                        'question': impact_text,
                        'context': {},
                        'priority': 'high'
                    })

        if not added:
            return questions

        # Add to front (high priority) but keep stable ordering otherwise.
        priority_order = {'high': 0, 'medium': 1, 'low': 2}
        combined = questions + added
        combined.sort(key=lambda q: priority_order.get(q.get('priority', 'low'), 3))
        return combined[:max_questions]
    
    def generate_questions(self, 
                          knowledge_graph: KnowledgeGraph,
                          dependency_graph: DependencyGraph,
                          max_questions: int = 10) -> List[Dict[str, Any]]:
        """
        Generate questions to denoise the complaint.
        
        Args:
            knowledge_graph: Current knowledge graph
            dependency_graph: Current dependency graph
            max_questions: Maximum number of questions to generate
            
        Returns:
            List of question dictionaries with type, question text, and context
        """
        questions = []
        
        # Get knowledge graph gaps
        kg_gaps = knowledge_graph.find_gaps()
        for gap in kg_gaps[:max_questions]:
            if gap['type'] == 'low_confidence_entity':
                questions.append({
                    'type': 'clarification',
                    'question': gap['suggested_question'],
                    'context': {
                        'entity_id': gap['entity_id'],
                        'entity_name': gap['entity_name'],
                        'confidence': gap['confidence']
                    },
                    'priority': 'medium'
                })
            elif gap['type'] == 'unsupported_claim':
                questions.append({
                    'type': 'evidence',
                    'question': gap['suggested_question'],
                    'context': {
                        'claim_id': gap['entity_id'],
                        'claim_name': gap['claim_name']
                    },
                    'priority': 'high'
                })
            elif gap['type'] == 'isolated_entity':
                questions.append({
                    'type': 'relationship',
                    'question': gap['suggested_question'],
                    'context': {
                        'entity_id': gap['entity_id'],
                        'entity_name': gap['entity_name']
                    },
                    'priority': 'low'
                })
            elif gap['type'] == 'missing_timeline':
                questions.append({
                    'type': 'timeline',
                    'question': gap['suggested_question'],
                    'context': {},
                    'priority': 'high'
                })
            elif gap['type'] == 'missing_responsible_party':
                questions.append({
                    'type': 'responsible_party',
                    'question': gap['suggested_question'],
                    'context': {},
                    'priority': 'high'
                })
        
        # Get dependency graph unsatisfied requirements
        unsatisfied = dependency_graph.find_unsatisfied_requirements()
        for req in unsatisfied[:max_questions - len(questions)]:
            missing_deps = req.get('missing_dependencies', [])
            for dep in missing_deps[:2]:  # Ask about first 2 missing deps
                questions.append({
                    'type': 'requirement',
                    'question': f"To support the claim '{req['node_name']}', can you provide information about: {dep['source_name']}?",
                    'context': {
                        'claim_id': req['node_id'],
                        'claim_name': req['node_name'],
                        'requirement_id': dep['source_node_id'],
                        'requirement_name': dep['source_name']
                    },
                    'priority': 'high'
                })
        
        # Sort by priority (baseline ordering)
        priority_order = {'high': 0, 'medium': 1, 'low': 2}
        questions.sort(key=lambda q: priority_order.get(q.get('priority', 'low'), 3))

        # Ensure we cover basic intake dimensions beyond evidence-only prompts.
        questions = self._ensure_standard_intake_questions(questions, max_questions)

        # Optional exploration/momentum policy (reorders questions only).
        questions = self._apply_exploration_and_momentum(questions)

        # Light empathy / framing tweaks (text-only; doesn't change structure).
        for q in questions:
            qtype = q.get('type', '')
            qtext = q.get('question', '')
            q['question'] = self._with_empathy(qtext, qtype)
        
        # Track questions in pool
        self.questions_pool.extend(questions[:max_questions])
        
        return questions[:max_questions]
    
    def process_answer(self, question: Dict[str, Any], answer: str,
                      knowledge_graph: KnowledgeGraph,
                      dependency_graph: Optional[DependencyGraph] = None) -> Dict[str, Any]:
        """
        Process an answer to a denoising question.
        
        Args:
            question: The question that was asked
            answer: The user's answer
            knowledge_graph: Knowledge graph to update
            dependency_graph: Optional dependency graph to update
            
        Returns:
            Information about what was updated
        """
        self.questions_asked.append({
            'question': question,
            'answer': answer
        })
        
        updates = {
            'entities_updated': 0,
            'relationships_added': 0,
            'requirements_satisfied': 0
        }
        
        question_type = question.get('type')
        context = question.get('context', {})
        
        if question_type == 'clarification':
            # Update entity with clarified information
            entity_id = context.get('entity_id')
            entity = knowledge_graph.get_entity(entity_id)
            if entity:
                entity.confidence = min(1.0, entity.confidence + 0.2)
                entity.attributes['clarification'] = answer
                updates['entities_updated'] += 1
        
        elif question_type == 'relationship':
            # Extract relationships from answer (simplified)
            # In production, use LLM to extract structured relationships
            entity_id = context.get('entity_id')
            if entity_id and len(answer) > 10:
                # Mark entity as having relationships described
                entity = knowledge_graph.get_entity(entity_id)
                if entity:
                    entity.attributes['relationship_described'] = True
                    updates['entities_updated'] += 1
            updates = self._update_responsible_parties_from_answer(answer, knowledge_graph, updates)

        elif question_type == 'responsible_party':
            updates = self._update_responsible_parties_from_answer(answer, knowledge_graph, updates)
        
        elif question_type == 'evidence':
            # Track evidence description
            claim_id = context.get('claim_id')
            entity = knowledge_graph.get_entity(claim_id)
            if entity:
                if 'evidence_descriptions' not in entity.attributes:
                    entity.attributes['evidence_descriptions'] = []
                entity.attributes['evidence_descriptions'].append(answer)
                updates['entities_updated'] += 1
            if not claim_id:
                claims = knowledge_graph.get_entities_by_type('claim')
                if len(claims) == 1:
                    claim_id = claims[0].id
            if claim_id and len(answer) > 10:
                snippet = self._short_description(answer, 120)
                evidence_name = f"Evidence: {self._short_description(answer, 80)}"
                evidence_entity, created = self._add_entity_if_missing(
                    knowledge_graph,
                    'evidence',
                    evidence_name,
                    {'description': snippet},
                    0.6
                )
                if created:
                    updates['entities_updated'] += 1
                if evidence_entity:
                    _, rel_created = self._add_relationship_if_missing(
                        knowledge_graph,
                        claim_id,
                        evidence_entity.id,
                        'supported_by',
                        0.6
                    )
                    if rel_created:
                        updates['relationships_added'] += 1

        elif question_type == 'timeline':
            dates = self._extract_date_strings(answer)
            if dates:
                claims = knowledge_graph.get_entities_by_type('claim')
                claim_id = claims[0].id if len(claims) == 1 else None
                for date_str in dates:
                    date_entity, created = self._add_entity_if_missing(
                        knowledge_graph,
                        'date',
                        date_str,
                        {},
                        0.7
                    )
                    if created:
                        updates['entities_updated'] += 1
                    if claim_id and date_entity:
                        _, rel_created = self._add_relationship_if_missing(
                            knowledge_graph,
                            claim_id,
                            date_entity.id,
                            'occurred_on',
                            0.6
                        )
                        if rel_created:
                            updates['relationships_added'] += 1
            elif answer and answer.strip():
                claims = knowledge_graph.get_entities_by_type('claim')
                claim_id = claims[0].id if len(claims) == 1 else None
                snippet = self._short_description(answer, 120)
                fact_name = f"Timeline detail: {self._short_description(answer, 60)}"
                fact_entity, created = self._add_entity_if_missing(
                    knowledge_graph,
                    'fact',
                    fact_name,
                    {'fact_type': 'timeline', 'description': snippet},
                    0.6
                )
                if created:
                    updates['entities_updated'] += 1
                if claim_id and fact_entity:
                    _, rel_created = self._add_relationship_if_missing(
                        knowledge_graph,
                        claim_id,
                        fact_entity.id,
                        'has_timeline_detail',
                        0.6
                    )
                    if rel_created:
                        updates['relationships_added'] += 1

        elif question_type in {'impact', 'remedy'}:
            if answer and answer.strip():
                claims = knowledge_graph.get_entities_by_type('claim')
                claim_id = claims[0].id if len(claims) == 1 else None
                snippet = self._short_description(answer, 120)
                if question_type == 'remedy':
                    fact_type = 'remedy'
                    fact_name = f"Requested remedy: {self._short_description(answer, 60)}"
                    rel_type = 'seeks_remedy'
                else:
                    fact_type = 'impact'
                    fact_name = f"Impact: {self._short_description(answer, 60)}"
                    rel_type = 'has_impact'
                fact_entity, created = self._add_entity_if_missing(
                    knowledge_graph,
                    'fact',
                    fact_name,
                    {'fact_type': fact_type, 'description': snippet},
                    0.6
                )
                if created:
                    updates['entities_updated'] += 1
                if claim_id and fact_entity:
                    _, rel_created = self._add_relationship_if_missing(
                        knowledge_graph,
                        claim_id,
                        fact_entity.id,
                        rel_type,
                        0.6
                    )
                    if rel_created:
                        updates['relationships_added'] += 1
                if question_type == 'impact' and self._contains_remedy_cue(answer):
                    remedy_name = f"Requested remedy: {self._short_description(answer, 60)}"
                    remedy_entity, remedy_created = self._add_entity_if_missing(
                        knowledge_graph,
                        'fact',
                        remedy_name,
                        {'fact_type': 'remedy', 'description': snippet},
                        0.55
                    )
                    if remedy_created:
                        updates['entities_updated'] += 1
                    if claim_id and remedy_entity:
                        _, rel_created = self._add_relationship_if_missing(
                            knowledge_graph,
                            claim_id,
                            remedy_entity.id,
                            'seeks_remedy',
                            0.55
                        )
                        if rel_created:
                            updates['relationships_added'] += 1
        
        elif question_type == 'requirement':
            # Mark requirement as addressed
            if dependency_graph:
                req_id = context.get('requirement_id')
                req_node = dependency_graph.get_node(req_id)
                if req_node and len(answer) > 10:
                    req_node.satisfied = True
                    req_node.confidence = 0.7
                    updates['requirements_satisfied'] += 1
        
        logger.info(f"Processed answer: {updates}")

        # Update momentum from observed “gain”.
        try:
            gain = self._compute_gain(updates)
            self._recent_gains.append(gain)
            # Cap memory.
            if len(self._recent_gains) > 50:
                self._recent_gains = self._recent_gains[-50:]
            qtype = question.get('type') if isinstance(question, dict) else 'unknown'
            self._update_momentum(str(qtype or 'unknown'), gain)
        except Exception:
            pass

        return updates
    
    def calculate_noise_level(self, 
                             knowledge_graph: KnowledgeGraph,
                             dependency_graph: DependencyGraph) -> float:
        """
        Calculate current noise/uncertainty level.
        
        Lower values indicate less noise (more complete, confident information).
        
        Args:
            knowledge_graph: Current knowledge graph
            dependency_graph: Current dependency graph
            
        Returns:
            Noise level from 0.0 (no noise) to 1.0 (maximum noise)
        """
        # Calculate knowledge graph confidence
        kg_confidence = 0.0
        if knowledge_graph.entities:
            total_confidence = sum(e.confidence for e in knowledge_graph.entities.values())
            kg_confidence = total_confidence / len(knowledge_graph.entities)
        
        # Calculate dependency satisfaction
        readiness = dependency_graph.get_claim_readiness()
        dep_satisfaction = readiness.get('overall_readiness', 0.0)
        
        # Calculate gap ratio
        kg_gaps = len(knowledge_graph.find_gaps())
        kg_entities = len(knowledge_graph.entities)
        gap_ratio = kg_gaps / max(kg_entities, 1)
        
        # Combine metrics (lower is better)
        noise = (
            (1.0 - kg_confidence) * 0.4 +  # 40% weight on entity confidence
            (1.0 - dep_satisfaction) * 0.4 +  # 40% weight on dependency satisfaction
            min(gap_ratio, 1.0) * 0.2  # 20% weight on gaps
        )
        
        return noise
    
    def is_exhausted(self) -> bool:
        """
        Check if we've exhausted the question pool.
        
        Returns:
            True if no more questions can be asked
        """
        return len(self.questions_pool) == 0 or len(self.questions_asked) > 50
    
    def generate_evidence_questions(self,
                                   knowledge_graph: KnowledgeGraph,
                                   dependency_graph: DependencyGraph,
                                   evidence_gaps: List[Dict[str, Any]],
                                   max_questions: int = 5) -> List[Dict[str, Any]]:
        """
        Generate denoising questions for evidence phase.
        
        Args:
            knowledge_graph: Current knowledge graph
            dependency_graph: Current dependency graph
            evidence_gaps: Identified evidence gaps
            max_questions: Maximum questions to generate
            
        Returns:
            List of evidence-focused denoising questions
        """
        questions = []
        
        # Questions about missing evidence
        for gap in evidence_gaps[:max_questions]:
            questions.append({
                'type': 'evidence_clarification',
                'question': f"Do you have evidence to support: {gap.get('name', 'this claim')}?",
                'context': {
                    'gap_id': gap.get('id'),
                    'claim_id': gap.get('related_claim'),
                    'gap_type': gap.get('type', 'missing_evidence')
                },
                'priority': 'high'
            })
        
        # Questions about evidence quality/completeness
        evidence_entities = knowledge_graph.get_entities_by_type('evidence')
        for evidence in evidence_entities[:max(0, max_questions - len(questions))]:
            if evidence.confidence < 0.7:
                questions.append({
                    'type': 'evidence_quality',
                    'question': f"Can you provide more details about this evidence: {evidence.name}?",
                    'context': {
                        'evidence_id': evidence.id,
                        'evidence_name': evidence.name,
                        'confidence': evidence.confidence
                    },
                    'priority': 'medium'
                })
        
        return questions[:max_questions]
    
    def generate_legal_matching_questions(self,
                                         matching_results: Dict[str, Any],
                                         max_questions: int = 5) -> List[Dict[str, Any]]:
        """
        Generate denoising questions for legal matching phase.
        
        Args:
            matching_results: Results from neurosymbolic matching
            max_questions: Maximum questions to generate
            
        Returns:
            List of legal-focused denoising questions
        """
        questions = []
        
        # Questions about unsatisfied legal requirements
        unmatched = matching_results.get('unmatched_requirements', [])
        for req in unmatched[:max_questions]:
            questions.append({
                'type': 'legal_requirement',
                'question': f"To satisfy the legal requirement '{req.get('name')}', can you provide: {req.get('missing_info', 'additional information')}?",
                'context': {
                    'requirement_id': req.get('id'),
                    'requirement_name': req.get('name'),
                    'legal_element': req.get('element_type')
                },
                'priority': 'high'
            })
        
        # Questions about weak matches
        weak_matches = [m for m in matching_results.get('matches', []) 
                       if m.get('confidence', 1.0) < 0.6]
        for match in weak_matches[:max(0, max_questions - len(questions))]:
            questions.append({
                'type': 'legal_strengthening',
                'question': f"Can you provide more information to strengthen the claim for: {match.get('claim_name')}?",
                'context': {
                    'claim_id': match.get('claim_id'),
                    'legal_requirement': match.get('requirement_name'),
                    'confidence': match.get('confidence')
                },
                'priority': 'medium'
            })
        
        return questions[:max_questions]
    
    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of denoising progress."""
        return {
            'questions_asked': len(self.questions_asked),
            'questions_remaining': len(self.questions_pool),
            'exhausted': self.is_exhausted()
        }
    
    def synthesize_complaint_summary(self,
                                    knowledge_graph: KnowledgeGraph,
                                    conversation_history: List[Dict[str, Any]],
                                    evidence_list: List[Dict[str, Any]] = None) -> str:
        """
        Synthesize a human-readable summary from knowledge graph, chat transcripts, 
        and evidence without exposing raw graph structures.
        
        This implements the denoising diffusion pattern by progressively refining
        the narrative from structured data.
        
        Args:
            knowledge_graph: The complaint knowledge graph
            conversation_history: List of conversation exchanges
            evidence_list: Optional list of evidence items
            
        Returns:
            Human-readable complaint summary
        """
        summary_parts = []
        
        # Extract key entities
        people = knowledge_graph.get_entities_by_type('person')
        organizations = knowledge_graph.get_entities_by_type('organization')
        claims = knowledge_graph.get_entities_by_type('claim')
        facts = knowledge_graph.get_entities_by_type('fact')
        
        # Build narrative introduction
        if people or organizations:
            summary_parts.append("## Parties Involved")
            for person in people[:5]:  # Limit to key people
                summary_parts.append(f"- {person.name}: {person.attributes.get('role', 'individual')}")
            for org in organizations[:5]:
                summary_parts.append(f"- {org.name}: {org.attributes.get('role', 'organization')}")
            summary_parts.append("")
        
        # Summarize the complaint nature
        if claims:
            summary_parts.append("## Nature of Complaint")
            for claim in claims:
                claim_type = claim.attributes.get('claim_type', 'general')
                description = claim.attributes.get('description', claim.name)
                summary_parts.append(f"- **{claim.name}** ({claim_type}): {description}")
            summary_parts.append("")
        
        # Key facts from graph
        if facts:
            summary_parts.append("## Key Facts")
            high_conf_facts = [f for f in facts if f.confidence > 0.7]
            for fact in high_conf_facts[:10]:  # Top 10 confident facts
                summary_parts.append(f"- {fact.name}")
            summary_parts.append("")
        
        # Evidence summary
        if evidence_list and len(evidence_list) > 0:
            summary_parts.append("## Available Evidence")
            for evidence in evidence_list[:10]:
                ename = evidence.get('name', 'Evidence item')
                etype = evidence.get('type', 'document')
                summary_parts.append(f"- {ename} ({etype})")
            summary_parts.append("")
        
        # Key insights from conversation
        if conversation_history and len(conversation_history) > 0:
            summary_parts.append("## Additional Context from Discussion")
            # Extract key clarifications
            clarifications = [msg for msg in conversation_history 
                            if msg.get('type') == 'response' and len(msg.get('content', '')) > 50]
            for clarif in clarifications[:5]:  # Top 5 meaningful clarifications
                content = clarif.get('content', '')[:200]  # Limit length
                if len(clarif.get('content', '')) > 200:
                    content += "..."
                summary_parts.append(f"- {content}")
            summary_parts.append("")
        
        # Completeness assessment
        kg_summary = knowledge_graph.summary()
        completeness = "high" if kg_summary['total_entities'] > 10 else "moderate" if kg_summary['total_entities'] > 5 else "developing"
        summary_parts.append(f"**Complaint Status:** Information gathering {completeness}ly complete with {kg_summary['total_entities']} key elements identified.")
        
        return "\n".join(summary_parts)
