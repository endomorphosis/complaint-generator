"""
Optimizer Module

Analyzes critic feedback and provides optimization recommendations.
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class OptimizationReport:
    """Report with optimization insights and recommendations."""
    timestamp: str
    num_sessions_analyzed: int
    
    # Aggregate metrics
    average_score: float
    score_trend: str  # improving, declining, stable
    
    # Analysis by component
    question_quality_avg: float
    information_extraction_avg: float
    empathy_avg: float
    efficiency_avg: float
    coverage_avg: float
    
    # Top issues
    common_weaknesses: List[str]
    common_strengths: List[str]
    
    # Recommendations
    recommendations: List[str]
    priority_improvements: List[str]

    # Graph diagnostics (used to steer graph population/reduction improvements)
    kg_sessions_with_data: int = 0
    dg_sessions_with_data: int = 0
    kg_sessions_empty: int = 0
    dg_sessions_empty: int = 0
    kg_avg_total_entities: Optional[float] = None
    kg_avg_total_relationships: Optional[float] = None
    kg_avg_gaps: Optional[float] = None
    dg_avg_total_nodes: Optional[float] = None
    dg_avg_total_dependencies: Optional[float] = None
    dg_avg_satisfaction_rate: Optional[float] = None
    kg_avg_entities_delta_per_iter: Optional[float] = None
    kg_avg_relationships_delta_per_iter: Optional[float] = None
    kg_avg_gaps_delta_per_iter: Optional[float] = None
    kg_sessions_gaps_not_reducing: int = 0
    
    # Detailed insights
    best_session_id: str = None
    worst_session_id: str = None
    best_score: float = 0.0
    worst_score: float = 1.0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            'timestamp': self.timestamp,
            'num_sessions_analyzed': self.num_sessions_analyzed,
            'average_score': self.average_score,
            'score_trend': self.score_trend,
            'question_quality_avg': self.question_quality_avg,
            'information_extraction_avg': self.information_extraction_avg,
            'empathy_avg': self.empathy_avg,
            'efficiency_avg': self.efficiency_avg,
            'coverage_avg': self.coverage_avg,
            'common_weaknesses': self.common_weaknesses,
            'common_strengths': self.common_strengths,
            'recommendations': self.recommendations,
            'priority_improvements': self.priority_improvements,
            'kg_sessions_with_data': self.kg_sessions_with_data,
            'dg_sessions_with_data': self.dg_sessions_with_data,
            'kg_sessions_empty': self.kg_sessions_empty,
            'dg_sessions_empty': self.dg_sessions_empty,
            'kg_avg_total_entities': self.kg_avg_total_entities,
            'kg_avg_total_relationships': self.kg_avg_total_relationships,
            'kg_avg_gaps': self.kg_avg_gaps,
            'dg_avg_total_nodes': self.dg_avg_total_nodes,
            'dg_avg_total_dependencies': self.dg_avg_total_dependencies,
            'dg_avg_satisfaction_rate': self.dg_avg_satisfaction_rate,
            'kg_avg_entities_delta_per_iter': self.kg_avg_entities_delta_per_iter,
            'kg_avg_relationships_delta_per_iter': self.kg_avg_relationships_delta_per_iter,
            'kg_avg_gaps_delta_per_iter': self.kg_avg_gaps_delta_per_iter,
            'kg_sessions_gaps_not_reducing': self.kg_sessions_gaps_not_reducing,
            'best_session_id': self.best_session_id,
            'worst_session_id': self.worst_session_id,
            'best_score': self.best_score,
            'worst_score': self.worst_score
        }


class Optimizer:
    """
    Analyzes critic feedback to provide optimization recommendations.
    
    The optimizer:
    - Aggregates scores across sessions
    - Identifies patterns in successes and failures
    - Generates actionable recommendations
    - Tracks improvement over time
    """
    
    def __init__(self):
        """Initialize optimizer."""
        self.history = []

    @staticmethod
    def _safe_float(value: Any) -> Optional[float]:
        try:
            if value is None:
                return None
            if isinstance(value, bool):
                return None
            return float(value)
        except Exception:
            return None

    def _extract_graph_metrics(self, result: Any) -> Tuple[Optional[int], Optional[int], Optional[int], Optional[int], Optional[float], Optional[int]]:
        """Return (kg_entities, kg_relationships, dg_nodes, dg_dependencies, dg_satisfaction_rate, kg_gaps)."""
        kg_entities = None
        kg_relationships = None
        kg_gaps = None
        dg_nodes = None
        dg_dependencies = None
        dg_satisfaction_rate = None

        kg_summary = getattr(result, "knowledge_graph_summary", None)
        if isinstance(kg_summary, dict):
            kg_entities = kg_summary.get("total_entities")
            kg_relationships = kg_summary.get("total_relationships")
            kg_gaps = kg_summary.get("gaps")

        dg_summary = getattr(result, "dependency_graph_summary", None)
        if isinstance(dg_summary, dict):
            dg_nodes = dg_summary.get("total_nodes")
            dg_dependencies = dg_summary.get("total_dependencies")
            dg_satisfaction_rate = self._safe_float(dg_summary.get("satisfaction_rate"))

        # Fall back to full graph dict snapshots if summaries are missing.
        kg_dict = getattr(result, "knowledge_graph", None)
        if (kg_entities is None or kg_relationships is None) and isinstance(kg_dict, dict):
            entities = kg_dict.get("entities")
            rels = kg_dict.get("relationships")
            if isinstance(entities, dict):
                kg_entities = len(entities)
            if isinstance(rels, dict):
                kg_relationships = len(rels)

        dg_dict = getattr(result, "dependency_graph", None)
        if (dg_nodes is None or dg_dependencies is None or dg_satisfaction_rate is None) and isinstance(dg_dict, dict):
            nodes = dg_dict.get("nodes")
            deps = dg_dict.get("dependencies")
            if isinstance(nodes, dict):
                dg_nodes = len(nodes)
                try:
                    satisfied = 0
                    for n in nodes.values():
                        if isinstance(n, dict) and n.get("satisfied") is True:
                            satisfied += 1
                    dg_satisfaction_rate = (satisfied / len(nodes)) if nodes else 0.0
                except Exception:
                    dg_satisfaction_rate = dg_satisfaction_rate
            if isinstance(deps, dict):
                dg_dependencies = len(deps)

        def _safe_int(v: Any) -> Optional[int]:
            try:
                if v is None:
                    return None
                if isinstance(v, bool):
                    return None
                return int(v)
            except Exception:
                return None

        return (
            _safe_int(kg_entities),
            _safe_int(kg_relationships),
            _safe_int(dg_nodes),
            _safe_int(dg_dependencies),
            dg_satisfaction_rate,
            _safe_int(kg_gaps),
        )

    def _extract_kg_dynamics(self, result: Any) -> Tuple[Optional[float], Optional[float], Optional[float], bool]:
        """Return (entities_delta_per_iter, relationships_delta_per_iter, gaps_delta_per_iter, gaps_not_reducing)."""
        final_state = getattr(result, "final_state", None)
        if not isinstance(final_state, dict):
            return None, None, None, False
        history = final_state.get("loss_history")
        if not isinstance(history, list) or len(history) < 2:
            # Fall back to convergence_history if present
            history = final_state.get("convergence_history")
        if not isinstance(history, list) or len(history) < 2:
            return None, None, None, False

        def _metric_at(idx: int) -> Dict[str, Any]:
            row = history[idx]
            if not isinstance(row, dict):
                return {}
            m = row.get("metrics")
            return m if isinstance(m, dict) else {}

        m0 = _metric_at(0)
        m1 = _metric_at(-1)
        iters = max(1, len(history) - 1)

        def _int(v: Any) -> Optional[int]:
            try:
                if v is None or isinstance(v, bool):
                    return None
                return int(v)
            except Exception:
                return None

        e0 = _int(m0.get("entities"))
        e1 = _int(m1.get("entities"))
        r0 = _int(m0.get("relationships"))
        r1 = _int(m1.get("relationships"))
        g0 = _int(m0.get("gaps"))
        g1 = _int(m1.get("gaps"))

        de = ((e1 - e0) / iters) if (isinstance(e0, int) and isinstance(e1, int)) else None
        dr = ((r1 - r0) / iters) if (isinstance(r0, int) and isinstance(r1, int)) else None
        dg = ((g1 - g0) / iters) if (isinstance(g0, int) and isinstance(g1, int)) else None
        gaps_not_reducing = bool(isinstance(g0, int) and isinstance(g1, int) and g1 >= g0)
        return de, dr, dg, gaps_not_reducing
    
    def analyze(self, results: List[Any]) -> OptimizationReport:
        """
        Analyze session results and generate optimization report.
        
        Args:
            results: List of SessionResult objects
            
        Returns:
            OptimizationReport with insights and recommendations
        """
        logger.info(f"Analyzing {len(results)} session results")
        
        # Filter successful results
        successful = [r for r in results if r.success and r.critic_score]
        
        if not successful:
            logger.warning("No successful results to analyze")
            return self._empty_report(len(results))
        
        # Calculate aggregate metrics
        scores = [r.critic_score.overall_score for r in successful]
        avg_score = sum(scores) / len(scores)
        
        question_quality_scores = [r.critic_score.question_quality for r in successful]
        info_extraction_scores = [r.critic_score.information_extraction for r in successful]
        empathy_scores = [r.critic_score.empathy for r in successful]
        efficiency_scores = [r.critic_score.efficiency for r in successful]
        coverage_scores = [r.critic_score.coverage for r in successful]
        
        # Find best and worst
        best_result = max(successful, key=lambda r: r.critic_score.overall_score)
        worst_result = min(successful, key=lambda r: r.critic_score.overall_score)

        # Aggregate graph metrics
        kg_entities_vals: List[int] = []
        kg_rels_vals: List[int] = []
        kg_gaps_vals: List[int] = []
        dg_nodes_vals: List[int] = []
        dg_deps_vals: List[int] = []
        dg_rate_vals: List[float] = []
        kg_entities_delta_vals: List[float] = []
        kg_rels_delta_vals: List[float] = []
        kg_gaps_delta_vals: List[float] = []
        kg_with = 0
        dg_with = 0
        kg_empty = 0
        dg_empty = 0
        kg_gaps_not_reducing = 0
        for r in successful:
            kg_e, kg_r, dg_n, dg_d, dg_rate, kg_gaps = self._extract_graph_metrics(r)
            d_e, d_r, d_g, not_reducing = self._extract_kg_dynamics(r)
            if not_reducing:
                kg_gaps_not_reducing += 1
            if isinstance(d_e, (int, float)):
                kg_entities_delta_vals.append(float(d_e))
            if isinstance(d_r, (int, float)):
                kg_rels_delta_vals.append(float(d_r))
            if isinstance(d_g, (int, float)):
                kg_gaps_delta_vals.append(float(d_g))
            if kg_e is not None or kg_r is not None:
                kg_with += 1
                if kg_e == 0:
                    kg_empty += 1
            if dg_n is not None or dg_d is not None:
                dg_with += 1
                if dg_n == 0:
                    dg_empty += 1
            if isinstance(kg_e, int):
                kg_entities_vals.append(kg_e)
            if isinstance(kg_r, int):
                kg_rels_vals.append(kg_r)
            if isinstance(kg_gaps, int):
                kg_gaps_vals.append(kg_gaps)
            if isinstance(dg_n, int):
                dg_nodes_vals.append(dg_n)
            if isinstance(dg_d, int):
                dg_deps_vals.append(dg_d)
            if isinstance(dg_rate, (int, float)):
                dg_rate_vals.append(float(dg_rate))

        def _avg_int(vals: List[int]) -> Optional[float]:
            if not vals:
                return None
            return sum(vals) / len(vals)

        def _avg_float(vals: List[float]) -> Optional[float]:
            if not vals:
                return None
            return sum(vals) / len(vals)
        
        # Aggregate feedback
        all_strengths = []
        all_weaknesses = []
        all_suggestions = []
        
        for result in successful:
            all_strengths.extend(result.critic_score.strengths)
            all_weaknesses.extend(result.critic_score.weaknesses)
            all_suggestions.extend(result.critic_score.suggestions)
        
        # Find most common
        common_strengths = self._most_common(all_strengths, top_n=5)
        common_weaknesses = self._most_common(all_weaknesses, top_n=5)
        
        # Generate recommendations
        recommendations = self._generate_recommendations(
            avg_score,
            question_quality_scores,
            info_extraction_scores,
            empathy_scores,
            efficiency_scores,
            coverage_scores,
            common_weaknesses,
            all_suggestions,
            graph_summary={
                "kg_sessions_with_data": kg_with,
                "dg_sessions_with_data": dg_with,
                "kg_sessions_empty": kg_empty,
                "dg_sessions_empty": dg_empty,
                "kg_avg_total_entities": _avg_int(kg_entities_vals),
                "kg_avg_total_relationships": _avg_int(kg_rels_vals),
                "kg_avg_gaps": _avg_int(kg_gaps_vals),
                "dg_avg_total_nodes": _avg_int(dg_nodes_vals),
                "dg_avg_total_dependencies": _avg_int(dg_deps_vals),
                "dg_avg_satisfaction_rate": _avg_float(dg_rate_vals),
                "kg_avg_entities_delta_per_iter": _avg_float(kg_entities_delta_vals),
                "kg_avg_relationships_delta_per_iter": _avg_float(kg_rels_delta_vals),
                "kg_avg_gaps_delta_per_iter": _avg_float(kg_gaps_delta_vals),
                "kg_sessions_gaps_not_reducing": kg_gaps_not_reducing,
            },
        )
        
        # Determine priority improvements
        priority_improvements = self._determine_priorities(
            question_quality_scores,
            info_extraction_scores,
            empathy_scores,
            efficiency_scores,
            coverage_scores
        )
        
        # Determine trend
        trend = self._determine_trend(scores)
        
        report = OptimizationReport(
            timestamp=datetime.utcnow().isoformat(),
            num_sessions_analyzed=len(successful),
            average_score=avg_score,
            score_trend=trend,
            question_quality_avg=sum(question_quality_scores) / len(question_quality_scores),
            information_extraction_avg=sum(info_extraction_scores) / len(info_extraction_scores),
            empathy_avg=sum(empathy_scores) / len(empathy_scores),
            efficiency_avg=sum(efficiency_scores) / len(efficiency_scores),
            coverage_avg=sum(coverage_scores) / len(coverage_scores),
            common_weaknesses=common_weaknesses,
            common_strengths=common_strengths,
            recommendations=recommendations,
            priority_improvements=priority_improvements,
            kg_sessions_with_data=kg_with,
            dg_sessions_with_data=dg_with,
            kg_sessions_empty=kg_empty,
            dg_sessions_empty=dg_empty,
            kg_avg_total_entities=_avg_int(kg_entities_vals),
            kg_avg_total_relationships=_avg_int(kg_rels_vals),
            kg_avg_gaps=_avg_int(kg_gaps_vals),
            dg_avg_total_nodes=_avg_int(dg_nodes_vals),
            dg_avg_total_dependencies=_avg_int(dg_deps_vals),
            dg_avg_satisfaction_rate=_avg_float(dg_rate_vals),
            kg_avg_entities_delta_per_iter=_avg_float(kg_entities_delta_vals),
            kg_avg_relationships_delta_per_iter=_avg_float(kg_rels_delta_vals),
            kg_avg_gaps_delta_per_iter=_avg_float(kg_gaps_delta_vals),
            kg_sessions_gaps_not_reducing=kg_gaps_not_reducing,
            best_session_id=best_result.session_id,
            worst_session_id=worst_result.session_id,
            best_score=best_result.critic_score.overall_score,
            worst_score=worst_result.critic_score.overall_score
        )
        
        self.history.append(report)
        logger.info(f"Analysis complete. Average score: {avg_score:.3f}, Trend: {trend}")
        
        return report
    
    def _most_common(self, items: List[str], top_n: int = 5) -> List[str]:
        """Find most common items."""
        if not items:
            return []
        
        from collections import Counter
        counter = Counter(items)
        return [item for item, count in counter.most_common(top_n)]
    
    def _generate_recommendations(self,
                                  avg_score: float,
                                  question_quality: List[float],
                                  info_extraction: List[float],
                                  empathy: List[float],
                                  efficiency: List[float],
                                  coverage: List[float],
                                  weaknesses: List[str],
                              suggestions: List[str],
                              graph_summary: Optional[Dict[str, Any]] = None) -> List[str]:
        """Generate actionable recommendations."""
        recommendations = []
        
        # Score-based recommendations
        if avg_score < 0.5:
            recommendations.append("Overall performance is below average. Focus on fundamental improvements.")
        elif avg_score < 0.7:
            recommendations.append("Performance is moderate. Targeted improvements can significantly boost quality.")
        else:
            recommendations.append("Performance is good. Focus on consistency and edge cases.")
        
        # Component-specific recommendations
        avg_question_quality = sum(question_quality) / len(question_quality)
        if avg_question_quality < 0.6:
            recommendations.append("Improve question formulation: make questions more specific and relevant.")
        
        avg_info_extraction = sum(info_extraction) / len(info_extraction)
        if avg_info_extraction < 0.6:
            recommendations.append("Enhance information extraction: ask follow-up questions when responses are vague.")
        
        avg_empathy = sum(empathy) / len(empathy)
        if avg_empathy < 0.6:
            recommendations.append("Increase empathy: acknowledge complainant's feelings and concerns.")
        
        avg_efficiency = sum(efficiency) / len(efficiency)
        if avg_efficiency < 0.6:
            recommendations.append("Improve efficiency: avoid repetitive questions and streamline the process.")
        
        avg_coverage = sum(coverage) / len(coverage)
        if avg_coverage < 0.6:
            recommendations.append("Expand topic coverage: ensure all important aspects are addressed.")

        # Graph-aware recommendations (to steer improvements in KG/DG population/reduction).
        if isinstance(graph_summary, dict):
            kg_with = int(graph_summary.get("kg_sessions_with_data") or 0)
            dg_with = int(graph_summary.get("dg_sessions_with_data") or 0)
            kg_empty = int(graph_summary.get("kg_sessions_empty") or 0)
            dg_empty = int(graph_summary.get("dg_sessions_empty") or 0)
            kg_avg_entities = self._safe_float(graph_summary.get("kg_avg_total_entities"))
            dg_avg_nodes = self._safe_float(graph_summary.get("dg_avg_total_nodes"))
            kg_avg_gaps = self._safe_float(graph_summary.get("kg_avg_gaps"))
            kg_d_entities = self._safe_float(graph_summary.get("kg_avg_entities_delta_per_iter"))
            kg_d_rels = self._safe_float(graph_summary.get("kg_avg_relationships_delta_per_iter"))
            kg_d_gaps = self._safe_float(graph_summary.get("kg_avg_gaps_delta_per_iter"))
            kg_not_reducing = int(graph_summary.get("kg_sessions_gaps_not_reducing") or 0)
            dg_avg_rate = self._safe_float(graph_summary.get("dg_avg_satisfaction_rate"))

            if kg_with == 0:
                recommendations.append(
                    "No knowledge graph data was captured. Ensure Phase 1 builds a KnowledgeGraph and the session extracts/saves knowledge_graph_summary."
                )
            elif kg_empty == kg_with:
                recommendations.append(
                    "All knowledge graphs are empty. Improve entity/relationship extraction in complaint_phases/knowledge_graph.py so downstream phases can reason over claims and facts."
                )
            elif kg_avg_entities is not None and kg_avg_entities < 2:
                recommendations.append(
                    "Knowledge graphs are very small on average. Consider adding lightweight rule-based extraction (dates/actors/employer/action) or LLM extraction to enrich the KG."
                )

            if dg_with == 0:
                recommendations.append(
                    "No dependency graph data was captured. Ensure Phase 1 builds a DependencyGraph and the session extracts/saves dependency_graph_summary."
                )
            elif dg_empty == dg_with:
                recommendations.append(
                    "All dependency graphs are empty. This often indicates missing/empty claims in the KG or claim extraction logic; verify claim entities and dg_builder.build_from_claims inputs."
                )
            elif dg_avg_nodes is not None and dg_avg_nodes < 2:
                recommendations.append(
                    "Dependency graphs are very small on average. Expand claim->requirement modeling so denoising can target missing legal elements and facts."
                )

            if kg_avg_gaps is not None and kg_avg_gaps >= 3:
                recommendations.append(
                    "Knowledge graph gap count is high on average. Improve gap-reduction logic in complaint_phases/denoiser.py and ensure process_answer updates entities/relationships meaningfully."
                )
            if kg_not_reducing > 0:
                recommendations.append(
                    f"In {kg_not_reducing} sessions, KG gaps did not reduce over iterations. Consider making denoiser.process_answer reduce gaps deterministically (e.g., marking gap items as addressed when answers supply the missing fields)."
                )
            if kg_d_entities is not None and kg_d_entities < 0.1:
                recommendations.append(
                    "Knowledge graph is not growing much per iteration. Consider extracting structured entities/relationships from denoising answers to enrich the KG over time."
                )
            if kg_d_rels is not None and kg_d_rels < 0.05:
                recommendations.append(
                    "Knowledge graph relationships are not increasing across iterations. Consider adding relationship updates when answers mention who/what/when/where/why links."
                )
            if kg_d_gaps is not None and kg_d_gaps >= 0.0:
                recommendations.append(
                    "KG gaps are not decreasing on average (or are increasing). Improve gap selection + answer processing so each turn reduces uncertainty."
                )
            if dg_avg_rate is not None and dg_avg_rate < 0.2:
                recommendations.append(
                    "Dependency satisfaction rate is very low on average. Consider having denoising answers mark requirements as satisfied or add evidence/fact nodes as they are provided."
                )
        
        # Add unique suggestions from critics
        unique_suggestions = list(set(suggestions))
        recommendations.extend(unique_suggestions[:3])  # Top 3 suggestions
        
        return recommendations
    
    def _determine_priorities(self,
                             question_quality: List[float],
                             info_extraction: List[float],
                             empathy: List[float],
                             efficiency: List[float],
                             coverage: List[float]) -> List[str]:
        """Determine priority improvements based on lowest scores."""
        components = {
            'question_quality': sum(question_quality) / len(question_quality),
            'information_extraction': sum(info_extraction) / len(info_extraction),
            'empathy': sum(empathy) / len(empathy),
            'efficiency': sum(efficiency) / len(efficiency),
            'coverage': sum(coverage) / len(coverage)
        }
        
        # Sort by score (lowest first)
        sorted_components = sorted(components.items(), key=lambda x: x[1])
        
        # Return bottom 3 as priorities
        priorities = []
        for component, score in sorted_components[:3]:
            if score < 0.7:  # Only if below threshold
                priorities.append(f"Improve {component.replace('_', ' ')}: current avg {score:.2f}")
        
        return priorities
    
    def _determine_trend(self, scores: List[float]) -> str:
        """Determine if scores are improving, declining, or stable."""
        if len(scores) < 3:
            return "insufficient_data"
        
        # Simple linear trend
        first_half = scores[:len(scores)//2]
        second_half = scores[len(scores)//2:]
        
        first_avg = sum(first_half) / len(first_half)
        second_avg = sum(second_half) / len(second_half)
        
        diff = second_avg - first_avg
        
        if diff > 0.05:
            return "improving"
        elif diff < -0.05:
            return "declining"
        else:
            return "stable"
    
    def _empty_report(self, num_sessions: int) -> OptimizationReport:
        """Create empty report when no successful sessions."""
        return OptimizationReport(
            timestamp=datetime.utcnow().isoformat(),
            num_sessions_analyzed=0,
            average_score=0.0,
            score_trend="no_data",
            question_quality_avg=0.0,
            information_extraction_avg=0.0,
            empathy_avg=0.0,
            efficiency_avg=0.0,
            coverage_avg=0.0,
            common_weaknesses=["All sessions failed"],
            common_strengths=[],
            recommendations=["Debug system failures before optimization"],
            priority_improvements=["Fix system stability"]
        )
    
    def get_history(self) -> List[OptimizationReport]:
        """Get optimization history."""
        return self.history.copy()
    
    def compare_reports(self, report1: OptimizationReport, report2: OptimizationReport) -> Dict[str, Any]:
        """
        Compare two optimization reports.
        
        Args:
            report1: Earlier report
            report2: Later report
            
        Returns:
            Dictionary with comparison metrics
        """
        return {
            'score_change': report2.average_score - report1.average_score,
            'question_quality_change': report2.question_quality_avg - report1.question_quality_avg,
            'info_extraction_change': report2.information_extraction_avg - report1.information_extraction_avg,
            'empathy_change': report2.empathy_avg - report1.empathy_avg,
            'efficiency_change': report2.efficiency_avg - report1.efficiency_avg,
            'coverage_change': report2.coverage_avg - report1.coverage_avg,
            'trend_change': f"{report1.score_trend} -> {report2.score_trend}"
        }
