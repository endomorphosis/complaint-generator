"""
Adversarial Harness Module

Orchestrates multiple adversarial sessions with parallel execution.
"""

import logging
from typing import Dict, Any, List, Callable, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
from datetime import datetime
import os
import inspect

from .session import AdversarialSession, SessionResult
from .complainant import Complainant, ComplaintContext
from .critic import Critic
from .seed_complaints import SeedComplaintLibrary

logger = logging.getLogger(__name__)


class AdversarialHarness:
    """
    Orchestrates multiple adversarial training sessions.
    
    Features:
    - Parallel execution using LLM router
    - Progress tracking
    - Result aggregation
    - Failure handling
    """
    
    def __init__(self,
                 llm_backend_complainant,
                 llm_backend_critic,
                 mediator_factory: Callable,
                 seed_library: SeedComplaintLibrary = None,
                 max_parallel: int = 4,
                 session_state_dir: str | None = None,
                 llm_backend_complainant_factory: Optional[Callable[..., Any]] = None,
                 llm_backend_critic_factory: Optional[Callable[..., Any]] = None):
        """
        Initialize adversarial harness.
        
        Args:
            llm_backend_complainant: LLM backend for complainant
            llm_backend_critic: LLM backend for critic
            mediator_factory: Factory function to create mediator instances
            seed_library: Optional seed complaint library
            max_parallel: Maximum parallel sessions
            session_state_dir: Optional directory to persist each session under
                as <session_state_dir>/<session_id>/{chat.jsonl,session.json}.
        """
        self.llm_backend_complainant = llm_backend_complainant
        self.llm_backend_critic = llm_backend_critic
        self.llm_backend_complainant_factory = llm_backend_complainant_factory
        self.llm_backend_critic_factory = llm_backend_critic_factory
        self.mediator_factory = mediator_factory
        self.seed_library = seed_library or SeedComplaintLibrary()
        self.max_parallel = max_parallel
        self.session_state_dir = session_state_dir
        
        self.results = []

    def _safe_session_id(self, text: str) -> str:
        allowed = []
        for ch in text:
            if ch.isalnum() or ch in ('-', '_', '.'):
                allowed.append(ch)
            else:
                allowed.append('_')
        return ''.join(allowed)

    def _get_session_dir(self, session_id: str) -> str | None:
        if not self.session_state_dir:
            return None
        safe = self._safe_session_id(session_id)
        return os.path.join(self.session_state_dir, safe)

    def _create_mediator_for_session(
        self,
        *,
        evidence_db_path: str | None,
        legal_authority_db_path: str | None,
        session_id: str | None = None,
        session_dir: str | None = None,
    ):
        """Call mediator_factory with optional per-session DB paths if supported."""
        try:
            sig = inspect.signature(self.mediator_factory)
            params = sig.parameters
            accepts_kwargs = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in params.values())
            kwargs: Dict[str, Any] = {}
            if accepts_kwargs or "evidence_db_path" in params:
                kwargs["evidence_db_path"] = evidence_db_path
            if accepts_kwargs or "legal_authority_db_path" in params:
                kwargs["legal_authority_db_path"] = legal_authority_db_path
            if session_id is not None and (accepts_kwargs or "session_id" in params):
                kwargs["session_id"] = session_id
            if session_dir is not None and (accepts_kwargs or "session_dir" in params):
                kwargs["session_dir"] = session_dir
            if kwargs:
                return self.mediator_factory(**kwargs)
        except Exception:
            pass
        return self.mediator_factory()

    def _persist_session(self, result: SessionResult) -> None:
        if not self.session_state_dir:
            return

        session_dir = self._get_session_dir(result.session_id)
        if not session_dir:
            return
        os.makedirs(session_dir, exist_ok=True)

        session_json_path = os.path.join(session_dir, 'session.json')
        chat_jsonl_path = os.path.join(session_dir, 'chat.jsonl')

        payload = result.to_dict()

        # Persist full graphs as separate JSON files to keep session.json manageable.
        kg = payload.pop("knowledge_graph", None)
        dg = payload.pop("dependency_graph", None)
        if isinstance(kg, dict):
            with open(os.path.join(session_dir, "knowledge_graph.json"), "w", encoding="utf-8") as f:
                json.dump(kg, f, ensure_ascii=False, indent=2)
        if isinstance(dg, dict):
            with open(os.path.join(session_dir, "dependency_graph.json"), "w", encoding="utf-8") as f:
                json.dump(dg, f, ensure_ascii=False, indent=2)

        # Record artifact paths (if present).
        artifacts: Dict[str, Any] = {}
        evidence_db = os.path.join(session_dir, "evidence.duckdb")
        legal_db = os.path.join(session_dir, "legal_authorities.duckdb")
        artifacts["evidence_duckdb_expected"] = os.path.abspath(evidence_db)
        artifacts["legal_authorities_duckdb_expected"] = os.path.abspath(legal_db)
        artifacts["evidence_duckdb_exists"] = os.path.isfile(evidence_db)
        artifacts["legal_authorities_duckdb_exists"] = os.path.isfile(legal_db)
        if os.path.isfile(evidence_db):
            artifacts["evidence_duckdb"] = os.path.abspath(evidence_db)
        if os.path.isfile(legal_db):
            artifacts["legal_authorities_duckdb"] = os.path.abspath(legal_db)
        kg_path = os.path.join(session_dir, "knowledge_graph.json")
        dg_path = os.path.join(session_dir, "dependency_graph.json")
        if os.path.isfile(kg_path):
            artifacts["knowledge_graph_json"] = os.path.abspath(kg_path)
        if os.path.isfile(dg_path):
            artifacts["dependency_graph_json"] = os.path.abspath(dg_path)
        payload["artifacts"] = artifacts

        with open(session_json_path, 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)

        # Persist conversation history as JSONL for easy streaming/grepping.
        # Note: history entries don't include timestamps; include ordering index.
        with open(chat_jsonl_path, 'w', encoding='utf-8') as f:
            for i, msg in enumerate(result.conversation_history or []):
                line = {
                    'session_id': result.session_id,
                    'i': i,
                    'role': msg.get('role', 'unknown'),
                    'type': msg.get('type', ''),
                    'content': msg.get('content', ''),
                }
                f.write(json.dumps(line, ensure_ascii=False) + '\n')

        logger.info('Session artifacts saved to %s', session_dir)
    
    def run_batch(self,
                  num_sessions: int = 10,
                  seed_complaints: List[Dict[str, Any]] = None,
                  personalities: List[str] = None,
                  max_turns_per_session: int = 10) -> List[SessionResult]:
        """
        Run a batch of adversarial sessions in parallel.
        
        Args:
            num_sessions: Number of sessions to run
            seed_complaints: Optional list of seed complaints (randomly selected if None)
            personalities: Optional list of personalities for complainants
            max_turns_per_session: Maximum turns per session
            
        Returns:
            List of SessionResults
        """
        logger.info(f"Starting batch of {num_sessions} sessions with {self.max_parallel} parallel")
        
        # Get seed complaints
        if seed_complaints is None:
            seed_complaints = self.seed_library.get_seed_complaints(count=num_sessions)
        elif len(seed_complaints) < num_sessions:
            # Cycle through provided seeds
            seed_complaints = (seed_complaints * ((num_sessions // len(seed_complaints)) + 1))[:num_sessions]
        
        # Get personalities
        if personalities is None:
            personalities = ['cooperative', 'defensive', 'vague', 'detailed', 'emotional']
        
        # Create session specs
        session_specs = []
        for i in range(num_sessions):
            session_id = f"session_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{i:03d}"
            seed = seed_complaints[i % len(seed_complaints)]
            personality = personalities[i % len(personalities)]
            
            session_specs.append({
                'session_id': session_id,
                'seed': seed,
                'personality': personality,
                'max_turns': max_turns_per_session
            })
        
        # Run sessions in parallel
        results = []
        with ThreadPoolExecutor(max_workers=self.max_parallel) as executor:
            # Submit all tasks
            future_to_spec = {
                executor.submit(self._run_single_session, spec): spec
                for spec in session_specs
            }
            
            # Collect results as they complete
            completed = 0
            for future in as_completed(future_to_spec):
                spec = future_to_spec[future]
                try:
                    result = future.result()
                    # Attach spec metadata for downstream optimization.
                    try:
                        if isinstance(result.seed_complaint, dict):
                            result.seed_complaint = {
                                **result.seed_complaint,
                                '_meta': {
                                    'personality': spec.get('personality'),
                                    'max_turns': spec.get('max_turns'),
                                }
                            }
                    except Exception:
                        pass

                    results.append(result)
                    self._persist_session(result)
                    completed += 1
                    
                    if result.success:
                        logger.info(f"Session {spec['session_id']} completed ({completed}/{num_sessions}). "
                                  f"Score: {result.critic_score.overall_score:.3f}")
                    else:
                        logger.warning(f"Session {spec['session_id']} failed ({completed}/{num_sessions}): "
                                     f"{result.error}")
                except Exception as e:
                    logger.error(f"Error in session {spec['session_id']}: {e}")
                    completed += 1
        
        self.results.extend(results)
        logger.info(f"Batch complete. {len([r for r in results if r.success])}/{num_sessions} successful")
        
        return results
    
    def _run_single_session(self, spec: Dict[str, Any]) -> SessionResult:
        """Run a single session (called in thread pool)."""
        try:
            session_dir = self._get_session_dir(spec['session_id'])
            if session_dir:
                os.makedirs(session_dir, exist_ok=True)

            complainant_backend = self.llm_backend_complainant
            if callable(self.llm_backend_complainant_factory):
                complainant_backend = self.llm_backend_complainant_factory(
                    session_id=spec['session_id'],
                    session_dir=session_dir,
                )

            critic_backend = self.llm_backend_critic
            if callable(self.llm_backend_critic_factory):
                critic_backend = self.llm_backend_critic_factory(
                    session_id=spec['session_id'],
                    session_dir=session_dir,
                )

            # Create instances for this session
            complainant = Complainant(
                complainant_backend,
                personality=spec['personality']
            )
            
            # Set context (maps personality to emotional_state/cooperation/context_depth)
            complainant.set_context(Complainant.build_default_context(spec['seed'], spec['personality']))
            
            critic = Critic(critic_backend)
            evidence_db_path = os.path.join(session_dir, "evidence.duckdb") if session_dir else None
            legal_authority_db_path = os.path.join(session_dir, "legal_authorities.duckdb") if session_dir else None

            # Proactively create valid DuckDB container files so they are always present
            # in the session folder (hooks will still initialize schemas when DuckDB is available).
            try:
                import duckdb  # type: ignore
                if evidence_db_path:
                    conn = duckdb.connect(evidence_db_path)
                    conn.close()
                if legal_authority_db_path:
                    conn = duckdb.connect(legal_authority_db_path)
                    conn.close()
            except Exception:
                pass

            # Create new mediator instance (thread-safe). If supported, use per-session DuckDB paths.
            mediator = self._create_mediator_for_session(
                evidence_db_path=evidence_db_path,
                legal_authority_db_path=legal_authority_db_path,
                session_id=spec['session_id'],
                session_dir=session_dir,
            )
            
            # Create and run session
            session = AdversarialSession(
                session_id=spec['session_id'],
                complainant=complainant,
                mediator=mediator,
                critic=critic,
                max_turns=spec['max_turns']
            )
            
            result = session.run(spec['seed'])
            return result
            
        except Exception as e:
            logger.error(f"Error running session {spec['session_id']}: {e}", exc_info=True)
            return SessionResult(
                session_id=spec['session_id'],
                timestamp=datetime.utcnow().isoformat(),
                seed_complaint=spec['seed'],
                initial_complaint_text="",
                conversation_history=[],
                num_questions=0,
                num_turns=0,
                final_state={},
                success=False,
                error=str(e)
            )
    
    def get_results(self) -> List[SessionResult]:
        """Get all results from this harness."""
        return self.results.copy()
    
    def get_successful_results(self) -> List[SessionResult]:
        """Get only successful results."""
        return [r for r in self.results if r.success]
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Get statistics across all results.
        
        Returns:
            Dictionary with aggregate statistics
        """
        if not self.results:
            return {'total_sessions': 0}
        
        successful = self.get_successful_results()
        
        if not successful:
            return {
                'total_sessions': len(self.results),
                'successful_sessions': 0,
                'failed_sessions': len(self.results)
            }
        
        scores = [r.critic_score.overall_score for r in successful]
        question_counts = [r.num_questions for r in successful]
        durations = [r.duration_seconds for r in successful]
        
        return {
            'total_sessions': len(self.results),
            'successful_sessions': len(successful),
            'failed_sessions': len(self.results) - len(successful),
            'average_score': sum(scores) / len(scores) if scores else 0,
            'min_score': min(scores) if scores else 0,
            'max_score': max(scores) if scores else 0,
            'average_questions': sum(question_counts) / len(question_counts) if question_counts else 0,
            'average_duration': sum(durations) / len(durations) if durations else 0,
            'score_distribution': self._score_distribution(scores)
        }
    
    def _score_distribution(self, scores: List[float]) -> Dict[str, int]:
        """Calculate score distribution."""
        if not scores:
            return {}
        
        bins = {
            '0.0-0.2': 0,
            '0.2-0.4': 0,
            '0.4-0.6': 0,
            '0.6-0.8': 0,
            '0.8-1.0': 0
        }
        
        for score in scores:
            if score < 0.2:
                bins['0.0-0.2'] += 1
            elif score < 0.4:
                bins['0.2-0.4'] += 1
            elif score < 0.6:
                bins['0.4-0.6'] += 1
            elif score < 0.8:
                bins['0.6-0.8'] += 1
            else:
                bins['0.8-1.0'] += 1
        
        return bins
    
    def save_results(self, filepath: str):
        """
        Save results to JSON file.
        
        Args:
            filepath: Path to save results
        """
        data = {
            'timestamp': datetime.utcnow().isoformat(),
            'statistics': self.get_statistics(),
            'results': [r.to_dict() for r in self.results]
        }
        
        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)
        
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)
        
        logger.info(f"Results saved to {filepath}")
